{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pinecone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m load_dotenv()\n\u001b[1;32m      8\u001b[0m environ\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPINECONE_ENVIRONMENT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m pinecone\u001b[39m.\u001b[39minit(\n\u001b[1;32m     10\u001b[0m     \u001b[39m# api_key=environ.get(\"PINECONE_API_KEY\"), \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m# environment=environ.get(\"PINECONE_ENVIRONMENT\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m index_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpinecone-hackathon-test\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pinecone' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from os import environ\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "environ.get(\"PINECONE_ENVIRONMENT\")\n",
    "pinecone.init(\n",
    "    # api_key=environ.get(\"PINECONE_API_KEY\"), \n",
    "    # environment=environ.get(\"PINECONE_ENVIRONMENT\")\n",
    ")\n",
    "index_name = 'pinecone-hackathon-test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pinecone.Index(index_name=index_name)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Pinecone(pinecone_index, embeddings.embed_query, \"text\")\n",
    "print(vectorstore)\n",
    "# print(pinecone_index.describe_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pinecone.describe_index(index_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How do you connect two nodes in reactflow?\"\n",
    "# docs = vectorstore.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"/\n",
    "You are a Javascript developer. You are to write javascript code to accomplish the following goal. The goal is delimitted by triple back ticks. Please use the sample code. The sample code is delimitted by pipes.\n",
    "\n",
    "|{sample_code}|\n",
    "\n",
    "\n",
    "```\n",
    "{query}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt = prompt_template.format(sample_code=sample_code, query=query)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "# prompt = \"How do you connect two nodes in reactflow?\"\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "# response = model.predict(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.llms import OpenAI\n",
    "# from langchain.text_splitter import CharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\n",
    "qa = ConversationalRetrievalChain.from_llm(model, retriever=retriever, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    query,\n",
    "    # \"What classes are derived from the Chain class?\",\n",
    "    # \"What classes and functions in the ./langchain/utilities/ forlder are not covered by unit tests?\",\n",
    "    # \"What one improvement do you propose in code in relation to the class herarchy for the Chain class?\",\n",
    "]\n",
    "chat_history = []\n",
    "\n",
    "for question in questions:\n",
    "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
    "    chat_history.append((question, result[\"answer\"]))\n",
    "    print(f\"-> **Question**: {question} \\n\")\n",
    "    print(f\"**Answer**: {result['answer']} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
